{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark processing with SageMaker Studio and local IDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds processing registry login to the build template:\n",
    "# - $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 571004829621)\n",
    "\n",
    "!cp /opt/conda/lib/python3.7/site-packages/sagemaker_studio_image_build/data/buildspec.template.yml /opt/conda/lib/python3.7/site-packages/sagemaker_studio_image_build/data/buildspec.template.yml~\n",
    "!patch -p0 /opt/conda/lib/python3.7/site-packages/sagemaker_studio_image_build/data/buildspec.template.yml < ./spark-processing-image/sm_docker_ecr.patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /opt/conda/lib/python3.7/site-packages/sagemaker_studio_image_build/data/buildspec.template.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./spark-processing-image; sm-docker build . --repository sagemaker-studio-spark-glue:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker create-image --image-name sagemaker-studio-spark-glue --role-arn arn:aws:iam::054035656282:role/service-role/AmazonSageMaker-ExecutionRole-20220215T072743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker create-image-version --image-name sagemaker-studio-spark-glue --base-image 054035656282.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-studio-spark-glue:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker describe-image-version --image-name sagemaker-studio-spark-glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker create-app-image-config --cli-input-json file://spark-processing-image/app-image-config-input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker update-app-image-config --app-image-config-name sagemaker-studio-spark-glue-config --cli-input-json file://spark-processing-image/app-image-config-input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker update-domain --cli-input-json file://spark-processing-image/update-domain-input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker describe-app-image-config --app-image-config-name sagemaker-studio-spark-glue-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker list-images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you should select Python3 (sagemaker-studio-spark-glue) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/lib/spark/python/lib/pyspark.zip')\n",
    "sys.path.append('/usr/lib/spark/python/lib/py4j-0.10.9-src.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark_estimate_pi.py\n",
    "\n",
    "Code sample is adopted and modified from https://docs.aws.amazon.com/code-samples/latest/catalog/python-emr-pyspark_estimate_pi.py.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyspark_estimate_pi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyspark_estimate_pi.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "\"\"\"\n",
    "Purpose\n",
    "\n",
    "Shows how to write a script that calculates pi by using a large number of random\n",
    "numbers run in parallel on an Amazon EMR cluster. This script is intended to be\n",
    "uploaded to an Amazon S3 bucket so it can be run as a job step.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from operator import add\n",
    "import random\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "\n",
    "def calculate_pi(spark, partitions, tries, output_uri):\n",
    "    \"\"\"\n",
    "    Calculates pi by testing a large number of random numbers against a unit circle\n",
    "    inscribed inside a square. The trials are partitioned so they can be run in\n",
    "    parallel on cluster instances.\n",
    "\n",
    "    :param partitions: The number of partitions to use for the calculation.\n",
    "    :param output_uri: The URI where the output is written, typically an Amazon S3\n",
    "                       bucket, such as 's3://example-bucket/pi-calc'.\n",
    "    \"\"\"\n",
    "    def calculate_hit(_):\n",
    "        x = random.randint(0, 256) / 256.0 * 2 - 1\n",
    "        y = random.randint(0, 256) / 256.0 * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "    logger.info(\n",
    "        \"Calculating pi with a total of %s tries in %s partitions.\", tries, partitions)\n",
    "    \n",
    "    hits = spark.sparkContext.parallelize(range(tries), partitions)\\\n",
    "        .map(calculate_hit)\\\n",
    "        .reduce(add)\n",
    "    pi = 4.0 * hits / tries\n",
    "    logger.info(\"%s tries and %s hits gives pi estimate of %s.\", tries, hits, pi)\n",
    "    df = spark.createDataFrame(\n",
    "        [(tries, hits, pi)], ['tries', 'hits', 'pi'])\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--partitions', default=2, type=int,\n",
    "        help=\"The number of parallel partitions to use when calculating pi.\")\n",
    "    parser.add_argument(\n",
    "        '--tries', default=0, type=int,\n",
    "        help=\"The number of tries.\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    if args.tries > 0:\n",
    "        spark = SparkSession.builder.appName(\"My PyPi\")\\\n",
    "            .config(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\\\n",
    "            .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\")\\\n",
    "            .config(\"hadoop.hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\")\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Test Glue Data Catalog\n",
    "        spark.sql('show databases').show()\n",
    "        \n",
    "        # Test S3 access\n",
    "        df = spark.read.parquet(\"s3://amazon-reviews-pds/parquet/product_category=Books\")\n",
    "        df.first()\n",
    "        \n",
    "        calculate_pi(spark, args.partitions, args.tries, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make sure the script is working locally in the notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i pyspark_estimate_pi.py  # load calculate_pi() without running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hadoop.hive.metastore.client.factory.class\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.client.factory.class\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-54137fa3-0809-4a31-88f2-c276f80f015d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.0.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.271 in central\n",
      ":: resolution report :: resolve 313ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.271 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.0.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-54137fa3-0809-4a31-88f2-c276f80f015d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/18ms)\n",
      "2022-07-22 12:24:43,200 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-07-22 12:24:44,915 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1-amzn-0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My PyPi</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=My PyPi>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Note: for Glue catalog to work Hive needs patching, see:\n",
    "#   https://github.com/awslabs/aws-glue-data-catalog-client-for-apache-hive-metastore#patching-apache-hive-and-installing-it-locally\n",
    "# Also Glue jars need to be installed from emr-apps repo (package aws-hm-client)\n",
    "\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.0.3\")\\\n",
    "    .set(\"spark.jars\", \"/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar\")\\\n",
    "    .set(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\")\\\n",
    "    .set(\"hadoop.hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\")\\\n",
    "    .set(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "    .set(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\\\n",
    "    .set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "sc = pyspark.SparkContext(\"local[4]\", \"My PyPi\", conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    namespace|\n",
      "+-------------+\n",
      "|      default|\n",
      "|glue-database|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Calculating pi with a total of 10000000 tries in 4 partitions.\n",
      "INFO: 10000000 tries and 7787180 hits gives pi estimate of 3.114872.            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 501 ms, sys: 44.6 ms, total: 546 ms\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = calculate_pi(spark, 4, 10_000_000, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tries</th>\n",
       "      <th>hits</th>\n",
       "      <th>pi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000000</td>\n",
       "      <td>7787180</td>\n",
       "      <td>3.114872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tries     hits        pi\n",
       "0  10000000  7787180  3.114872"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hadoop.hive.metastore.client.factory.class\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.client.factory.class\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3ba0876e-0c07-4538-bd84-f82663f4365e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.0.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.271 in central\n",
      ":: resolution report :: resolve 331ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.271 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.0.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3ba0876e-0c07-4538-bd84-f82663f4365e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/18ms)\n",
      "2022-07-22 09:40:11,337 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-07-22 09:40:12,942 INFO spark.SparkContext: Running Spark version 3.0.1-amzn-0.1\n",
      "2022-07-22 09:40:12,989 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-07-22 09:40:12,991 INFO resource.ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "2022-07-22 09:40:12,992 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-07-22 09:40:12,992 INFO spark.SparkContext: Submitted application: My PyPi\n",
      "2022-07-22 09:40:13,068 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2022-07-22 09:40:13,068 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2022-07-22 09:40:13,068 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2022-07-22 09:40:13,068 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2022-07-22 09:40:13,068 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2022-07-22 09:40:13,377 INFO util.Utils: Successfully started service 'sparkDriver' on port 33855.\n",
      "2022-07-22 09:40:13,454 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2022-07-22 09:40:13,511 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2022-07-22 09:40:13,545 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2022-07-22 09:40:13,546 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2022-07-22 09:40:13,551 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2022-07-22 09:40:13,574 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-87b7578c-e891-4a6d-a2ad-4bb80996b2b1\n",
      "2022-07-22 09:40:13,615 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "2022-07-22 09:40:13,644 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2022-07-22 09:40:13,828 INFO util.log: Logging initialized @4875ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2022-07-22 09:40:13,915 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_312-b07\n",
      "2022-07-22 09:40:13,951 INFO server.Server: Started @5000ms\n",
      "2022-07-22 09:40:13,987 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-07-22 09:40:14,001 INFO server.AbstractConnector: Started ServerConnector@5bffd05d{HTTP/1.1,[http/1.1]}{127.0.0.1:4041}\n",
      "2022-07-22 09:40:14,002 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "2022-07-22 09:40:14,038 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fc842b3{/jobs,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,045 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1180c504{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,046 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4deae0f4{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,055 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6675a2b1{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,057 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b140f06{/stages,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,062 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cd5e6ad{/stages/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,064 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c5529ad{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,069 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ca435be{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,074 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a0f876{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,078 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b348bcb{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,081 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75785301{/storage,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,085 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e52f6b5{/storage/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,087 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c1f62a6{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,090 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e01904d{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,093 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34dd6d6e{/environment,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,096 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38471dc2{/environment/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,097 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4543dcbf{/executors,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,101 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c9c8f58{/executors/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,106 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e26b6ce{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,109 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23183ccc{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,123 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14f3f9ec{/static,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,125 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c3c89a1{/,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,130 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67a138cd{/api,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,133 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f9eaecd{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,135 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a031a21{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:14,139 INFO ui.SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4041\n",
      "2022-07-22 09:40:14,179 INFO spark.SparkContext: Added JAR file:///usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar at spark://localhost:33855/jars/aws-glue-datacatalog-spark-client.jar with timestamp 1658482814178\n",
      "2022-07-22 09:40:14,180 INFO spark.SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar at spark://localhost:33855/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar with timestamp 1658482814180\n",
      "2022-07-22 09:40:14,180 INFO spark.SparkContext: Added JAR file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar at spark://localhost:33855/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar with timestamp 1658482814180\n",
      "2022-07-22 09:40:14,194 INFO spark.SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar with timestamp 1658482814192\n",
      "2022-07-22 09:40:14,203 INFO util.Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/org.apache.hadoop_hadoop-aws-3.0.3.jar\n",
      "2022-07-22 09:40:14,231 INFO spark.SparkContext: Added file file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar at file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar with timestamp 1658482814231\n",
      "2022-07-22 09:40:14,232 INFO util.Utils: Copying /root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar\n",
      "2022-07-22 09:40:14,765 INFO executor.Executor: Starting executor ID driver on host localhost\n",
      "2022-07-22 09:40:14,827 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44645.\n",
      "2022-07-22 09:40:14,828 INFO netty.NettyBlockTransferService: Server created on localhost:44645\n",
      "2022-07-22 09:40:14,830 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2022-07-22 09:40:14,843 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 44645, None)\n",
      "2022-07-22 09:40:14,851 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:44645 with 366.3 MiB RAM, BlockManagerId(driver, localhost, 44645, None)\n",
      "2022-07-22 09:40:14,858 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 44645, None)\n",
      "2022-07-22 09:40:14,859 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 44645, None)\n",
      "2022-07-22 09:40:15,101 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28bb61b1{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:15,492 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/spark-warehouse/').\n",
      "2022-07-22 09:40:15,494 INFO internal.SharedState: Warehouse path is 'file:/root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/spark-warehouse/'.\n",
      "2022-07-22 09:40:15,516 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e601e0c{/SQL,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:15,519 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57ebc837{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:15,521 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e902843{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:15,523 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bd20b74{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:15,527 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@410746a4{/static/sql,null,AVAILABLE,@Spark}\n",
      "2022-07-22 09:40:18,743 INFO conf.HiveConf: Found configuration file null\n",
      "2022-07-22 09:40:18,768 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 2.3.7-amzn-2 using Spark classes.\n",
      "2022-07-22 09:40:18,858 INFO conf.HiveConf: Found configuration file null\n",
      "2022-07-22 09:40:19,288 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/6464291e-3069-4031-a13d-6fca5c36c813\n",
      "2022-07-22 09:40:19,296 INFO session.SessionState: Created local directory: /tmp/root/6464291e-3069-4031-a13d-6fca5c36c813\n",
      "2022-07-22 09:40:19,307 INFO session.SessionState: Created HDFS directory: /tmp/hive/root/6464291e-3069-4031-a13d-6fca5c36c813/_tmp_space.db\n",
      "2022-07-22 09:40:19,321 INFO client.HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is file:/root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/spark-warehouse/\n",
      "2022-07-22 09:40:23,945 INFO codegen.CodeGenerator: Code generated in 294.32508 ms\n",
      "2022-07-22 09:40:24,149 INFO codegen.CodeGenerator: Code generated in 11.77796 ms\n",
      "2022-07-22 09:40:24,174 INFO codegen.CodeGenerator: Code generated in 12.112927 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    namespace|\n",
      "+-------------+\n",
      "|      default|\n",
      "|glue-database|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 09:40:25,934 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\n",
      "2022-07-22 09:40:29,302 INFO datasources.InMemoryFileIndex: It took 873 ms to list leaf files for 1 paths.\n",
      "2022-07-22 09:40:30,130 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2022-07-22 09:40:30,164 INFO scheduler.DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2022-07-22 09:40:30,171 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2022-07-22 09:40:30,176 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-07-22 09:40:30,182 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-07-22 09:40:30,205 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-07-22 09:40:30,323 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 87.1 KiB, free 366.2 MiB)\n",
      "2022-07-22 09:40:30,371 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.9 KiB, free 366.2 MiB)\n",
      "2022-07-22 09:40:30,377 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:44645 (size: 30.9 KiB, free: 366.3 MiB)\n",
      "2022-07-22 09:40:30,385 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1263\n",
      "2022-07-22 09:40:30,425 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2022-07-22 09:40:30,428 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "2022-07-22 09:40:30,516 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes)\n",
      "2022-07-22 09:40:30,545 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "2022-07-22 09:40:30,550 INFO executor.Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar with timestamp 1658482814192\n",
      "2022-07-22 09:40:30,574 INFO util.Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar has been previously copied to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/org.apache.hadoop_hadoop-aws-3.0.3.jar\n",
      "2022-07-22 09:40:30,584 INFO executor.Executor: Fetching file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar with timestamp 1658482814231\n",
      "2022-07-22 09:40:30,681 INFO util.Utils: /root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar has been previously copied to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar\n",
      "2022-07-22 09:40:30,687 INFO executor.Executor: Fetching spark://localhost:33855/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar with timestamp 1658482814180\n",
      "2022-07-22 09:40:30,741 INFO client.TransportClientFactory: Successfully created connection to localhost/127.0.0.1:33855 after 40 ms (0 ms spent in bootstraps)\n",
      "2022-07-22 09:40:30,755 INFO util.Utils: Fetching spark://localhost:33855/jars/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/fetchFileTemp3745673737610770171.tmp\n",
      "2022-07-22 09:40:31,183 INFO util.Utils: /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/fetchFileTemp3745673737610770171.tmp has been previously copied to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar\n",
      "2022-07-22 09:40:31,200 INFO executor.Executor: Adding file:/tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/com.amazonaws_aws-java-sdk-bundle-1.11.271.jar to class loader\n",
      "2022-07-22 09:40:31,200 INFO executor.Executor: Fetching spark://localhost:33855/jars/aws-glue-datacatalog-spark-client.jar with timestamp 1658482814178\n",
      "2022-07-22 09:40:31,201 INFO util.Utils: Fetching spark://localhost:33855/jars/aws-glue-datacatalog-spark-client.jar to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/fetchFileTemp6432360133969828381.tmp\n",
      "2022-07-22 09:40:31,213 INFO executor.Executor: Adding file:/tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/aws-glue-datacatalog-spark-client.jar to class loader\n",
      "2022-07-22 09:40:31,213 INFO executor.Executor: Fetching spark://localhost:33855/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar with timestamp 1658482814180\n",
      "2022-07-22 09:40:31,219 INFO util.Utils: Fetching spark://localhost:33855/jars/org.apache.hadoop_hadoop-aws-3.0.3.jar to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/fetchFileTemp7758127447771213499.tmp\n",
      "2022-07-22 09:40:31,221 INFO util.Utils: /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/fetchFileTemp7758127447771213499.tmp has been previously copied to /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/org.apache.hadoop_hadoop-aws-3.0.3.jar\n",
      "2022-07-22 09:40:31,231 INFO executor.Executor: Adding file:/tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/userFiles-418f28f9-9289-4177-abbd-a21ad255d7a4/org.apache.hadoop_hadoop-aws-3.0.3.jar to class loader\n",
      "2022-07-22 09:40:31,995 INFO s3a.S3AInputStream: Switching to Random IO seek policy\n",
      "2022-07-22 09:40:32,422 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2392 bytes result sent to driver\n",
      "2022-07-22 09:40:32,434 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1951 ms on localhost (executor driver) (1/1)\n",
      "2022-07-22 09:40:32,439 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2022-07-22 09:40:32,455 INFO scheduler.DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.204 s\n",
      "2022-07-22 09:40:32,466 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-07-22 09:40:32,475 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2022-07-22 09:40:32,483 INFO scheduler.DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.349221 s\n",
      "2022-07-22 09:40:32,641 INFO datasources.FileSourceStrategy: Pruning directories with: \n",
      "2022-07-22 09:40:32,646 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2022-07-22 09:40:32,647 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2022-07-22 09:40:32,651 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\n",
      "2022-07-22 09:40:32,806 INFO codegen.CodeGenerator: Code generated in 76.134207 ms\n",
      "2022-07-22 09:40:32,852 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 321.6 KiB, free 365.9 MiB)\n",
      "2022-07-22 09:40:32,875 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 365.8 MiB)\n",
      "2022-07-22 09:40:32,880 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:44645 (size: 29.7 KiB, free: 366.2 MiB)\n",
      "2022-07-22 09:40:32,884 INFO spark.SparkContext: Created broadcast 1 from first at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:75\n",
      "2022-07-22 09:40:32,923 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 90, prefetch: true\n",
      "2022-07-22 09:40:32,934 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,80), (5 fileSplits,2))\n",
      "2022-07-22 09:40:32,976 INFO spark.SparkContext: Starting job: first at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:75\n",
      "2022-07-22 09:40:32,981 INFO scheduler.DAGScheduler: Got job 1 (first at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:75) with 1 output partitions\n",
      "2022-07-22 09:40:32,981 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (first at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:75)\n",
      "2022-07-22 09:40:32,981 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-07-22 09:40:32,982 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-07-22 09:40:32,983 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at first at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:75), which has no missing parents\n",
      "2022-07-22 09:40:33,028 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.4 KiB, free 365.8 MiB)\n",
      "2022-07-22 09:40:33,033 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.8 MiB)\n",
      "2022-07-22 09:40:33,034 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:44645 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "2022-07-22 09:40:33,040 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1263\n",
      "2022-07-22 09:40:33,044 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at first at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:75) (first 15 tasks are for partitions Vector(0))\n",
      "2022-07-22 09:40:33,044 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "2022-07-22 09:40:33,052 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7819 bytes)\n",
      "2022-07-22 09:40:33,055 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "2022-07-22 09:40:33,140 INFO datasources.AsyncFileDownloader: TID: 1 - Number of files to download: 1, unique file paths: 1\n",
      "2022-07-22 09:40:33,153 INFO datasources.AsyncFileDownloader: TID: 1 - Downloading file s3://amazon-reviews-pds/parquet/product_category=Books/part-00000-495c48e6-96d6-4650-aa65-3c36a3516ddd.c000.snappy.parquet, start: 0, length: 134217728\n",
      "2022-07-22 09:40:33,416 INFO s3a.S3AInputStream: Switching to Random IO seek policy\n",
      "2022-07-22 09:40:34,328 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on localhost:44645 in memory (size: 30.9 KiB, free: 366.3 MiB)\n",
      "2022-07-22 09:40:38,398 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "2022-07-22 09:40:38,493 INFO datasources.AsyncFileDownloader: TID: 1 - Downloaded file s3://amazon-reviews-pds/parquet/product_category=Books/part-00000-495c48e6-96d6-4650-aa65-3c36a3516ddd.c000.snappy.parquet, start: 0, length: 134217728, Elapsed time: 5335671micros\n",
      "2022-07-22 09:40:38,500 INFO datasources.FileScanRDD: TID: 1 - Got next file: path: s3://amazon-reviews-pds/parquet/product_category=Books/part-00000-495c48e6-96d6-4650-aa65-3c36a3516ddd.c000.snappy.parquet, range: 0-134217728, partition values: [empty row], isDataPresent: true. Elapsed time: 5332454 micros\n",
      "2022-07-22 09:40:38,504 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: s3://amazon-reviews-pds/parquet/product_category=Books/part-00000-495c48e6-96d6-4650-aa65-3c36a3516ddd.c000.snappy.parquet, range: 0-134217728, partition values: [empty row], isDataPresent: true\n",
      "2022-07-22 09:40:39,114 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2670 bytes result sent to driver\n",
      "2022-07-22 09:40:39,124 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6075 ms on localhost (executor driver) (1/1)\n",
      "2022-07-22 09:40:39,134 INFO scheduler.DAGScheduler: ResultStage 1 (first at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:75) finished in 6.143 s\n",
      "2022-07-22 09:40:39,134 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-07-22 09:40:39,141 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2022-07-22 09:40:39,144 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2022-07-22 09:40:39,145 INFO scheduler.DAGScheduler: Job 1 finished: first at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:75, took 6.167660 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Calculating pi with a total of 10000000 tries in 4 partitions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 09:40:39,277 INFO spark.SparkContext: Starting job: reduce at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:42\n",
      "2022-07-22 09:40:39,279 INFO scheduler.DAGScheduler: Got job 2 (reduce at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:42) with 4 output partitions\n",
      "2022-07-22 09:40:39,279 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (reduce at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:42)\n",
      "2022-07-22 09:40:39,279 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-07-22 09:40:39,280 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-07-22 09:40:39,281 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at reduce at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:42), which has no missing parents\n",
      "2022-07-22 09:40:39,285 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.5 KiB, free 365.9 MiB)\n",
      "2022-07-22 09:40:39,287 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 365.9 MiB)\n",
      "2022-07-22 09:40:39,288 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:44645 (size: 4.2 KiB, free: 366.3 MiB)\n",
      "2022-07-22 09:40:39,288 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1263\n",
      "2022-07-22 09:40:39,289 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 2 (PythonRDD[7] at reduce at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:42) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "2022-07-22 09:40:39,289 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 4 tasks\n",
      "2022-07-22 09:40:39,295 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7333 bytes)\n",
      "2022-07-22 09:40:39,296 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7333 bytes)\n",
      "2022-07-22 09:40:39,296 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "2022-07-22 09:40:39,310 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 3)\n",
      "2022-07-22 09:40:54,685 INFO python.PythonRunner: Times: total = 15351, boot = 609, init = 58, finish = 14684\n",
      "2022-07-22 09:40:54,695 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 1509 bytes result sent to driver\n",
      "2022-07-22 09:40:54,711 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4, localhost, executor driver, partition 2, PROCESS_LOCAL, 7333 bytes)\n",
      "2022-07-22 09:40:54,711 INFO executor.Executor: Running task 2.0 in stage 2.0 (TID 4)\n",
      "2022-07-22 09:40:54,722 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 15428 ms on localhost (executor driver) (1/4)\n",
      "2022-07-22 09:40:54,727 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48191\n",
      "2022-07-22 09:40:54,738 INFO python.PythonRunner: Times: total = 15403, boot = 600, init = 64, finish = 14739\n",
      "2022-07-22 09:40:54,759 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 3). 1509 bytes result sent to driver\n",
      "2022-07-22 09:40:54,769 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5, localhost, executor driver, partition 3, PROCESS_LOCAL, 7333 bytes)\n",
      "2022-07-22 09:40:54,770 INFO executor.Executor: Running task 3.0 in stage 2.0 (TID 5)\n",
      "2022-07-22 09:40:54,774 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 15479 ms on localhost (executor driver) (2/4)\n",
      "2022-07-22 09:41:09,284 INFO python.PythonRunner: Times: total = 14568, boot = 5, init = 12, finish = 14551\n",
      "2022-07-22 09:41:09,294 INFO executor.Executor: Finished task 2.0 in stage 2.0 (TID 4). 1509 bytes result sent to driver\n",
      "2022-07-22 09:41:09,299 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 14587 ms on localhost (executor driver) (3/4)\n",
      "2022-07-22 09:41:09,334 INFO python.PythonRunner: Times: total = 14561, boot = -5, init = 28, finish = 14538\n",
      "2022-07-22 09:41:09,340 INFO executor.Executor: Finished task 3.0 in stage 2.0 (TID 5). 1509 bytes result sent to driver\n",
      "2022-07-22 09:41:09,343 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 14575 ms on localhost (executor driver) (4/4)\n",
      "2022-07-22 09:41:09,343 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2022-07-22 09:41:09,344 INFO scheduler.DAGScheduler: ResultStage 2 (reduce at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:42) finished in 30.061 s\n",
      "2022-07-22 09:41:09,345 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-07-22 09:41:09,345 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "2022-07-22 09:41:09,345 INFO scheduler.DAGScheduler: Job 2 finished: reduce at /root/plexperiments-p-led4ckjkoxzx/sagemaker-plexperiments-modelbuild/./pyspark_estimate_pi.py:42, took 30.067663 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 10000000 tries and 7788020 hits gives pi estimate of 3.115208.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 09:41:10,003 INFO spark.SparkContext: Invoking stop() from shutdown hook\n",
      "2022-07-22 09:41:10,012 INFO server.AbstractConnector: Stopped Spark@5bffd05d{HTTP/1.1,[http/1.1]}{127.0.0.1:4041}\n",
      "2022-07-22 09:41:10,014 INFO ui.SparkUI: Stopped Spark web UI at http://localhost:4041\n",
      "2022-07-22 09:41:10,043 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2022-07-22 09:41:10,066 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2022-07-22 09:41:10,067 INFO storage.BlockManager: BlockManager stopped\n",
      "2022-07-22 09:41:10,084 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2022-07-22 09:41:10,091 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2022-07-22 09:41:10,111 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "2022-07-22 09:41:10,111 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "2022-07-22 09:41:10,112 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a50f17d3-d955-43f7-918e-b8b0e13a0866\n",
      "2022-07-22 09:41:10,119 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318\n",
      "2022-07-22 09:41:10,123 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ef7d0df0-0c77-4d9c-819c-e89c7f283318/pyspark-82d3a840-2581-4d93-970e-f93da162ccda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 307 ms, sys: 54 ms, total: 361 ms\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%sh\n",
    "\n",
    "# NOTE: for some reason --conf doesn't override fs.s3a.aws.credentials.provider\n",
    "#   when passed as a command line option, so we set it in code \n",
    "\n",
    "spark-submit \\\n",
    "    --conf \"spark.jars=/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar\" \\\n",
    "    --conf \"spark.jars.packages=org.apache.hadoop:hadoop-aws:3.0.3\" \\\n",
    "    --conf \"spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\" \\\n",
    "    --conf \"hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" \\\n",
    "    --conf \"hadoop.hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\" \\\n",
    "    ./pyspark_estimate_pi.py --partitions 4 --tries 10000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to sumbit the bigger processing jobs for 200M records. If you're running the notebook on your local machine, replace the placeholder `<<YOUR_IAM_ROLE_ARN_FOR_SAGEMAKER_HERE>>` with the IAM role ARN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "\n",
    "role = sagemaker.get_execution_role() \\\n",
    "    if os.path.exists('/opt/ml/metadata/resource-metadata.json') \\\n",
    "    else \"<<YOUR_IAM_ROLE_ARN_FOR_SAGEMAKER_HERE>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-pi\",\n",
    "    framework_version=\"3.0\",\n",
    "    role=role,\n",
    "    instance_count=4,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1800,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"pyspark_estimate_pi.py\",\n",
    "    arguments=[\n",
    "        \"--partitions\", \"16\",\n",
    "        \"--tries\", \"200_000_000\"\n",
    "    ],\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can test your script in AWS Glue, which is a serverless data integration service with extra useful ETL capabilities like visualization of Spark-specific metrics in AWS Console and the catalog of data sets, but more loosely integrated with capabilities of SageMaker like SageMaker Pipelines and MLOps and gives less choice in the processing instance types.\n",
    "\n",
    "Make sure that your IAM role also has the trust relationship with Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source = spark_processor.latest_job.inputs[0].source\n",
    "input_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$input_source\" \"$role\"\n",
    "\n",
    "aws glue create-job \\\n",
    "    --name \"Glue Evaluate Pi\" \\\n",
    "    --role $2 \\\n",
    "    --command '{\"Name\": \"glueetl\", \"ScriptLocation\": \"'$1'\", \"PythonVersion\": \"3\"}' \\\n",
    "    --max-retries 0 \\\n",
    "    --timeout 30 \\\n",
    "    --worker-type \"G.1X\" \\\n",
    "    --number-of-workers 2 \\\n",
    "    --glue-version \"3.0\" \\\n",
    "    --default-arguments '{\"--partitions\": \"1\", \"--tries\": \"200_000_000\", \"--job-language\": \"python\", \"--class\": \"GlueApp\", \"--enable-metrics\": \"true\", \"--enable-glue-datacatalog\": \"true\", \"--enable-job-insights\": \"true\", \"--enable-spark-ui\": \"true\", \"--enable-continuous-cloudwatch-log\": \"true\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws glue start-job-run --job-name \"Glue Evaluate Pi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws glue delete-job --job-name \"Glue Evaluate Pi\""
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (sagemaker-studio-spark-glue/latest)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:054035656282:image/sagemaker-studio-spark-glue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
